{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HIgPKJfKuW6B"},"outputs":[],"source":["!git clone https://github.com/gaopengcuhk/Tip-Adapter.git\n","%cd Tip-Adapter\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","source":["修改Tip-Adapter中的main.py"],"metadata":{"id":"kgTAL-uYU9jw"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"YTP_zhnmwfLE","executionInfo":{"status":"ok","timestamp":1710409110129,"user_tz":-480,"elapsed":1377,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}}},"outputs":[],"source":["!cp -r /content/drive/MyDrive/main.py  /content/Tip-Adapter"]},{"cell_type":"markdown","source":["将处理好的数据集复制到指定路径"],"metadata":{"id":"PEZf-F9xeLdO"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"PU7kr_eXuEJP","executionInfo":{"status":"ok","timestamp":1710411250996,"user_tz":-480,"elapsed":96615,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}}},"outputs":[],"source":["!cp -r /content/drive/MyDrive/data_process/data_process_1000/  /content/drive/MyDrive/ucf101/UCF-101-midframes"]},{"cell_type":"markdown","source":["100张图训练"],"metadata":{"id":"nxBUfiHQgMb0"}},{"cell_type":"markdown","source":["注意：训练前需要在/content/Tip-Adapter/configs/ucf101.yaml中补充root_path"],"metadata":{"id":"Ye2rD1MOWNqf"}},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0 python main.py --config configs/ucf101.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCzKeHz1c3se","executionInfo":{"status":"ok","timestamp":1710411964346,"user_tz":-480,"elapsed":264519,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"c50a480e-766f-419c-a5dc-7971e2bc8054"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running configs.\n","{'root_path': '/content/drive/MyDrive', 'load_cache': False, 'load_pre_feat': False, 'search_hp': True, 'search_scale': [7, 3], 'search_step': [200, 20], 'init_beta': 1, 'init_alpha': 3, 'dataset': 'ucf101', 'shots': 16, 'backbone': 'RN50', 'lr': 0.001, 'augment_epoch': 10, 'train_epoch': 20, 'cache_dir': './caches/ucf101'} \n","\n","Preparing dataset.\n","Reading split from /content/drive/MyDrive/ucf101/split_zhou_UCF101.json\n","Creating a 16-shot dataset\n","\n","Getting textual features as CLIP's classifier.\n","\n","Constructing cache model by few-shot visual features and labels.\n","Augment Epoch: 0 / 10\n","100% 1/1 [00:47<00:00, 47.37s/it]\n","Augment Epoch: 1 / 10\n","100% 1/1 [00:00<00:00,  1.23it/s]\n","Augment Epoch: 2 / 10\n","100% 1/1 [00:00<00:00,  1.23it/s]\n","Augment Epoch: 3 / 10\n","100% 1/1 [00:00<00:00,  1.26it/s]\n","Augment Epoch: 4 / 10\n","100% 1/1 [00:00<00:00,  1.26it/s]\n","Augment Epoch: 5 / 10\n","100% 1/1 [00:00<00:00,  1.24it/s]\n","Augment Epoch: 6 / 10\n","100% 1/1 [00:00<00:00,  1.24it/s]\n","Augment Epoch: 7 / 10\n","100% 1/1 [00:00<00:00,  1.20it/s]\n","Augment Epoch: 8 / 10\n","100% 1/1 [00:00<00:00,  1.18it/s]\n","Augment Epoch: 9 / 10\n","100% 1/1 [00:00<00:00,  1.16it/s]\n","\n","Loading visual features and labels from val set.\n","100% 1/1 [00:20<00:00, 20.72s/it]\n","\n","Loading visual features and labels from test set.\n","  0% 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 31/31 [02:37<00:00,  5.07s/it]\n","\n","-------- Searching hyperparameters on the val set. --------\n","\n","**** Zero-shot CLIP's val accuracy: 100.00. ****\n","\n","**** Tip-Adapter's val accuracy: 95.00. ****\n","\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 100.00\n","\n","After searching, the best accuarcy: 100.00.\n","\n","\n","-------- Evaluating on the test set. --------\n","\n","**** Zero-shot CLIP's test accuracy: 47.37. ****\n","\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter's test metrics: Precision: 0.4045, Recall: 0.8006, F1 Score: 0.5324. ****\n","\n","**** Tip-Adapter's test accuracy: 47.42. ****\n","\n","Train Epoch: 0 / 20\n","100% 1/1 [00:01<00:00,  1.15s/it]\n","LR: 0.000994, Acc: 1.0000 (80.0/80), Loss: 0.0016\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 1 / 20\n","100% 1/1 [00:01<00:00,  1.02s/it]\n","LR: 0.000976, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 2 / 20\n","100% 1/1 [00:00<00:00,  1.04it/s]\n","LR: 0.000946, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 3 / 20\n","100% 1/1 [00:00<00:00,  1.01it/s]\n","LR: 0.000905, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 4 / 20\n","100% 1/1 [00:00<00:00,  1.03it/s]\n","LR: 0.000854, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 5 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000794, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 6 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000727, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 7 / 20\n","100% 1/1 [00:00<00:00,  1.04it/s]\n","LR: 0.000655, Acc: 1.0000 (80.0/80), Loss: 0.0005\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 8 / 20\n","100% 1/1 [00:00<00:00,  1.05it/s]\n","LR: 0.000578, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 9 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000500, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 10 / 20\n","100% 1/1 [00:00<00:00,  1.03it/s]\n","LR: 0.000422, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 11 / 20\n","100% 1/1 [00:00<00:00,  1.04it/s]\n","LR: 0.000345, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 12 / 20\n","100% 1/1 [00:00<00:00,  1.00it/s]\n","LR: 0.000273, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 13 / 20\n","100% 1/1 [00:01<00:00,  1.00s/it]\n","LR: 0.000206, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 14 / 20\n","100% 1/1 [00:00<00:00,  1.04it/s]\n","LR: 0.000146, Acc: 1.0000 (80.0/80), Loss: 0.0007\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 15 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000095, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 16 / 20\n","100% 1/1 [00:00<00:00,  1.11it/s]\n","LR: 0.000054, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 17 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000024, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 18 / 20\n","100% 1/1 [00:00<00:00,  1.07it/s]\n","LR: 0.000006, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","Train Epoch: 19 / 20\n","100% 1/1 [00:01<00:00,  1.01s/it]\n","LR: 0.000000, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n","**** After fine-tuning, Tip-Adapter-F's best test accuracy: 49.19, at epoch: 3. ****\n","\n","\n","-------- Searching hyperparameters on the val set. --------\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 100.00\n","\n","After searching, the best accuarcy: 100.00.\n","\n","\n","-------- Evaluating on the test set. --------\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter-F's test metrics: Precision: 0.4045, Recall: 0.8006, F1 Score: 0.5324. ****\n","\n","**** Tip-Adapter-F's test accuracy: 49.19. ****\n","\n"]}]},{"cell_type":"markdown","source":["500张图训练"],"metadata":{"id":"imXagcUTOEck"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43836,"status":"ok","timestamp":1710142071562,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"},"user_tz":-480},"id":"xE1KtjoYsae9","outputId":"b2f35482-56da-4bc4-c968-72838eac8959"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running configs.\n","{'root_path': '/content/drive/MyDrive', 'load_cache': False, 'load_pre_feat': False, 'search_hp': True, 'search_scale': [7, 3], 'search_step': [200, 20], 'init_beta': 1, 'init_alpha': 3, 'dataset': 'ucf101', 'shots': 16, 'backbone': 'RN50', 'lr': 0.001, 'augment_epoch': 10, 'train_epoch': 20, 'cache_dir': './caches/ucf101'} \n","\n","Preparing dataset.\n","Reading split from /content/drive/MyDrive/ucf101/split_zhou_UCF101.json\n","Creating a 16-shot dataset\n","\n","Getting textual features as CLIP's classifier.\n","\n","Constructing cache model by few-shot visual features and labels.\n","Augment Epoch: 0 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:01<00:00,  1.28s/it]\n","Augment Epoch: 1 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.31it/s]\n","Augment Epoch: 2 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.27it/s]\n","Augment Epoch: 3 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.30it/s]\n","Augment Epoch: 4 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.28it/s]\n","Augment Epoch: 5 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.36it/s]\n","Augment Epoch: 6 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.35it/s]\n","Augment Epoch: 7 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.35it/s]\n","Augment Epoch: 8 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.29it/s]\n","Augment Epoch: 9 / 10\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.19it/s]\n","\n","Loading visual features and labels from val set.\n","100% 2/2 [00:01<00:00,  1.96it/s]\n","\n","Loading visual features and labels from test set.\n","  0% 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 31/31 [00:06<00:00,  5.16it/s]\n","\n","-------- Searching hyperparameters on the val set. --------\n","\n","**** Zero-shot CLIP's val accuracy: 94.00. ****\n","\n","**** Tip-Adapter's val accuracy: 99.00. ****\n","\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 94.00\n","New best setting, beta: 0.10, alpha: 1.70; accuracy: 95.00\n","New best setting, beta: 0.10, alpha: 1.84; accuracy: 96.00\n","New best setting, beta: 0.10, alpha: 1.98; accuracy: 97.00\n","New best setting, beta: 0.13, alpha: 2.85; accuracy: 98.00\n","New best setting, beta: 0.17, alpha: 2.71; accuracy: 99.00\n","\n","After searching, the best accuarcy: 99.00.\n","\n","\n","-------- Evaluating on the test set. --------\n","\n","**** Zero-shot CLIP's test accuracy: 47.37. ****\n","\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter's test metrics: Precision: 0.4093, Recall: 0.8223, F1 Score: 0.5450. ****\n","\n","**** Tip-Adapter's test accuracy: 48.73. ****\n","\n","Train Epoch: 0 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.00it/s]\n","LR: 0.000994, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 1 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.11it/s]\n","LR: 0.000976, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 2 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.11it/s]\n","LR: 0.000946, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 3 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.12it/s]\n","LR: 0.000905, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 4 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.10it/s]\n","LR: 0.000854, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 5 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.10it/s]\n","LR: 0.000794, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 6 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.14it/s]\n","LR: 0.000727, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 7 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000655, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 8 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000578, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 9 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.09it/s]\n","LR: 0.000500, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 10 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.12it/s]\n","LR: 0.000422, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 11 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.14it/s]\n","LR: 0.000345, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 12 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.09it/s]\n","LR: 0.000273, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 13 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.15it/s]\n","LR: 0.000206, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 14 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.11it/s]\n","LR: 0.000146, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 15 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.13it/s]\n","LR: 0.000095, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 16 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.12it/s]\n","LR: 0.000054, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 17 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.09it/s]\n","LR: 0.000024, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 18 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000006, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","Train Epoch: 19 / 20\n","  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 1/1 [00:00<00:00,  1.00it/s]\n","LR: 0.000000, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n","**** After fine-tuning, Tip-Adapter-F's best test accuracy: 49.09, at epoch: 0. ****\n","\n","\n","-------- Searching hyperparameters on the val set. --------\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 94.00\n","New best setting, beta: 0.10, alpha: 1.70; accuracy: 95.00\n","New best setting, beta: 0.10, alpha: 1.84; accuracy: 96.00\n","New best setting, beta: 0.10, alpha: 1.98; accuracy: 97.00\n","New best setting, beta: 0.13, alpha: 2.85; accuracy: 98.00\n","New best setting, beta: 0.17, alpha: 2.71; accuracy: 99.00\n","\n","After searching, the best accuarcy: 99.00.\n","\n","\n","-------- Evaluating on the test set. --------\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter-F's test metrics: Precision: 0.4093, Recall: 0.8223, F1 Score: 0.5450. ****\n","\n","**** Tip-Adapter-F's test accuracy: 49.09. ****\n","\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0 python main.py --config configs/ucf101.yaml"]},{"cell_type":"markdown","source":["1000张图训练"],"metadata":{"id":"hnq7MUKJOI8P"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pBv-l-sV7rg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710141806115,"user_tz":-480,"elapsed":46170,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"a2e26885-c5e5-4843-a16f-63e8ce3c2d61"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running configs.\n","{'root_path': '/content/drive/MyDrive', 'load_cache': False, 'load_pre_feat': False, 'search_hp': True, 'search_scale': [7, 3], 'search_step': [200, 20], 'init_beta': 1, 'init_alpha': 3, 'dataset': 'ucf101', 'shots': 16, 'backbone': 'RN50', 'lr': 0.001, 'augment_epoch': 10, 'train_epoch': 20, 'cache_dir': './caches/ucf101'} \n","\n","Preparing dataset.\n","Reading split from /content/drive/MyDrive/ucf101/split_zhou_UCF101.json\n","Creating a 16-shot dataset\n","\n","Getting textual features as CLIP's classifier.\n","\n","Constructing cache model by few-shot visual features and labels.\n","Augment Epoch: 0 / 10\n","100% 1/1 [00:01<00:00,  1.31s/it]\n","Augment Epoch: 1 / 10\n","100% 1/1 [00:00<00:00,  1.17it/s]\n","Augment Epoch: 2 / 10\n","100% 1/1 [00:00<00:00,  1.15it/s]\n","Augment Epoch: 3 / 10\n","100% 1/1 [00:00<00:00,  1.25it/s]\n","Augment Epoch: 4 / 10\n","100% 1/1 [00:00<00:00,  1.22it/s]\n","Augment Epoch: 5 / 10\n","100% 1/1 [00:00<00:00,  1.16it/s]\n","Augment Epoch: 6 / 10\n","100% 1/1 [00:00<00:00,  1.15it/s]\n","Augment Epoch: 7 / 10\n","100% 1/1 [00:00<00:00,  1.19it/s]\n","Augment Epoch: 8 / 10\n","100% 1/1 [00:00<00:00,  1.19it/s]\n","Augment Epoch: 9 / 10\n","100% 1/1 [00:00<00:00,  1.18it/s]\n","\n","Loading visual features and labels from val set.\n","100% 4/4 [00:01<00:00,  2.97it/s]\n","\n","Loading visual features and labels from test set.\n","  0% 0/31 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100% 31/31 [00:05<00:00,  5.42it/s]\n","\n","-------- Searching hyperparameters on the val set. --------\n","\n","**** Zero-shot CLIP's val accuracy: 95.00. ****\n","\n","**** Tip-Adapter's val accuracy: 99.00. ****\n","\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 95.00\n","New best setting, beta: 0.10, alpha: 0.68; accuracy: 96.00\n","New best setting, beta: 0.10, alpha: 1.41; accuracy: 96.50\n","New best setting, beta: 0.10, alpha: 2.56; accuracy: 97.00\n","New best setting, beta: 0.13, alpha: 2.85; accuracy: 97.50\n","New best setting, beta: 0.17, alpha: 2.42; accuracy: 98.00\n","New best setting, beta: 0.20, alpha: 2.85; accuracy: 98.50\n","New best setting, beta: 0.24, alpha: 2.71; accuracy: 99.00\n","New best setting, beta: 0.34, alpha: 2.85; accuracy: 99.50\n","\n","After searching, the best accuarcy: 99.50.\n","\n","\n","-------- Evaluating on the test set. --------\n","\n","**** Zero-shot CLIP's test accuracy: 47.37. ****\n","\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter's test metrics: Precision: 0.4105, Recall: 0.8273, F1 Score: 0.5478. ****\n","\n","**** Tip-Adapter's test accuracy: 49.04. ****\n","\n","Train Epoch: 0 / 20\n","100% 1/1 [00:01<00:00,  1.10s/it]\n","LR: 0.000994, Acc: 0.9875 (79.0/80), Loss: 0.0591\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 1 / 20\n","100% 1/1 [00:00<00:00,  1.05it/s]\n","LR: 0.000976, Acc: 1.0000 (80.0/80), Loss: 0.0037\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 2 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000946, Acc: 1.0000 (80.0/80), Loss: 0.0023\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 3 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000905, Acc: 1.0000 (80.0/80), Loss: 0.0029\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 4 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000854, Acc: 1.0000 (80.0/80), Loss: 0.0050\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 5 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000794, Acc: 1.0000 (80.0/80), Loss: 0.0005\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 6 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000727, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 7 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000655, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 8 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000578, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 9 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000500, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 10 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000422, Acc: 1.0000 (80.0/80), Loss: 0.0007\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 11 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000345, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 12 / 20\n","100% 1/1 [00:01<00:00,  1.02s/it]\n","LR: 0.000273, Acc: 1.0000 (80.0/80), Loss: 0.0003\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 13 / 20\n","100% 1/1 [00:00<00:00,  1.02it/s]\n","LR: 0.000206, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 14 / 20\n","100% 1/1 [00:00<00:00,  1.05it/s]\n","LR: 0.000146, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 15 / 20\n","100% 1/1 [00:00<00:00,  1.08it/s]\n","LR: 0.000095, Acc: 1.0000 (80.0/80), Loss: 0.0002\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 16 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000054, Acc: 1.0000 (80.0/80), Loss: 0.0000\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 17 / 20\n","100% 1/1 [00:00<00:00,  1.06it/s]\n","LR: 0.000024, Acc: 1.0000 (80.0/80), Loss: 0.0005\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 18 / 20\n","100% 1/1 [00:00<00:00,  1.05it/s]\n","LR: 0.000006, Acc: 1.0000 (80.0/80), Loss: 0.0001\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","Train Epoch: 19 / 20\n","100% 1/1 [00:00<00:00,  1.04it/s]\n","LR: 0.000000, Acc: 1.0000 (80.0/80), Loss: 0.0025\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n","**** After fine-tuning, Tip-Adapter-F's best test accuracy: 49.14, at epoch: 0. ****\n","\n","\n","-------- Searching hyperparameters on the val set. --------\n","New best setting, beta: 0.10, alpha: 0.10; accuracy: 95.00\n","New best setting, beta: 0.10, alpha: 0.53; accuracy: 95.50\n","New best setting, beta: 0.10, alpha: 0.68; accuracy: 96.00\n","New best setting, beta: 0.10, alpha: 1.41; accuracy: 96.50\n","New best setting, beta: 0.10, alpha: 2.56; accuracy: 97.00\n","New best setting, beta: 0.13, alpha: 2.71; accuracy: 97.50\n","New best setting, beta: 0.13, alpha: 2.85; accuracy: 98.00\n","New best setting, beta: 0.20, alpha: 2.71; accuracy: 99.00\n","New best setting, beta: 0.24, alpha: 2.71; accuracy: 99.50\n","\n","After searching, the best accuarcy: 99.50.\n","\n","\n","-------- Evaluating on the test set. --------\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","**** Tip-Adapter-F's test metrics: Precision: 0.4110, Recall: 0.8273, F1 Score: 0.5480. ****\n","\n","**** Tip-Adapter-F's test accuracy: 49.14. ****\n","\n"]}],"source":["!CUDA_VISIBLE_DEVICES=0 python main.py --config configs/ucf101.yaml"]},{"cell_type":"markdown","source":["从data文件夹中抽取train并于test合并"],"metadata":{"id":"zFff6ZSgO1pg"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148757,"status":"ok","timestamp":1710139796952,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"},"user_tz":-480},"id":"LTnFoS63umsX","outputId":"85f86e84-1c0d-41b1-838c-9657843519dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["完成从train中抽取10张图片并与test合并到/content/drive/MyDrive/data_process/data_process_10\n","完成从train中抽取100张图片并与test合并到/content/drive/MyDrive/data_process/data_process_100\n","完成从train中抽取500张图片并与test合并到/content/drive/MyDrive/data_process/data_process_500\n","完成从train中抽取1000张图片并与test合并到/content/drive/MyDrive/data_process/data_process_1000\n"]}],"source":["import os\n","import shutil\n","from random import sample\n","\n","def copy_and_merge_selected_samples(train_dir, test_dir, num_samples_each, dest_dir):\n","    if os.path.exists(dest_dir):\n","        shutil.rmtree(dest_dir)\n","    os.makedirs(dest_dir)\n","\n","    # 获取train和test中所有的类别，使用集合来去除重复项\n","    train_categories = set(os.listdir(train_dir))\n","    test_categories = set(os.listdir(test_dir))\n","    all_categories = train_categories.union(test_categories)\n","\n","    for category in all_categories:\n","        category_dir = os.path.join(dest_dir, category)\n","        if not os.path.exists(category_dir):\n","            os.makedirs(category_dir)\n","\n","        # 如果当前类别在train中存在，则从train中随机选择图片并复制到新文件夹\n","        if category in train_categories:\n","            train_category_dir = os.path.join(train_dir, category)\n","            train_filenames = os.listdir(train_category_dir)\n","            selected_filenames = sample(train_filenames, min(num_samples_each, len(train_filenames)))\n","            for filename in selected_filenames:\n","                src_path = os.path.join(train_category_dir, filename)\n","                dest_path = os.path.join(category_dir, filename)\n","                shutil.copy(src_path, dest_path)\n","\n","        # 复制test中的图片到新文件夹，包括others类别\n","        test_category_dir = os.path.join(test_dir, category)\n","        if os.path.exists(test_category_dir):  # 确保test目录存在\n","            for filename in os.listdir(test_category_dir):\n","                src_path = os.path.join(test_category_dir, filename)\n","                dest_path = os.path.join(category_dir, f\"test_{filename}\")\n","                shutil.copy(src_path, dest_path)\n","\n","# 定义文件夹路径\n","train_dir = '/content/drive/MyDrive/data/train'  # 替换为你的train文件夹路径\n","test_dir = '/content/drive/MyDrive/data/test'    # 替换为你的test文件夹路径\n","dest_dir = '/content/drive/MyDrive/data_process/data_process'  # 替换为你想要存放合并后文件的新文件夹路径\n","\n","for num_samples in [10, 100, 500, 1000]:\n","    new_dest_dir = f\"{dest_dir}_{num_samples}\"\n","    copy_and_merge_selected_samples(train_dir, test_dir, int(num_samples/5), new_dest_dir)\n","    print(f\"完成从train中抽取{num_samples}张图片并与test合并到{new_dest_dir}\")"]},{"cell_type":"markdown","source":["生成json标注文件"],"metadata":{"id":"0GuVkHH-R0-3"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gpcr0t7bIaN-","executionInfo":{"status":"ok","timestamp":1710411096311,"user_tz":-480,"elapsed":21959,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"eee2f67f-eba5-4e11-9c48-a572de3b4fc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["JSON文件已创建: /content/drive/MyDrive/output_data.json\n"]}],"source":["import os\n","import json\n","import random\n","\n","# 定义包含了合并后的图片的文件夹路径\n","images_folder_path = '/content/drive/MyDrive/data_process/data_process_1000'\n","\n","# 初始化一个字典来保存训练、验证和测试数据\n","data = {'train': [], 'val': [], 'test': []}\n","\n","# 类别标签，为每个类别假定一个数字标签\n","category_labels = {\n","    \"pet cat\": 0,\n","    \"computer\": 1,\n","    \"tomato\": 2,\n","    \"paper-cut\": 3,\n","    \"dumpling\": 4,\n","    \"others\": 5\n","}\n","\n","# 为了保持结果的一致性，可以设置一个随机种子\n","random.seed(42)\n","\n","# 遍历每个类别，收集图片路径\n","for category, label in category_labels.items():\n","    category_path = os.path.join(images_folder_path, category)\n","    image_paths = []\n","    test_image_paths = []\n","\n","    if os.path.isdir(category_path):\n","        for image_name in os.listdir(category_path):\n","            if image_name.endswith(('.jpg', '.jpeg', '.png')):\n","                # image_path = os.path.join(images_folder_path, category, image_name)\n","                image_path = os.path.join(category, image_name)\n","                # 检查是否为测试集中的图片\n","                if image_name.startswith('test_'):\n","                    test_image_paths.append([image_path, label, category])\n","                else:\n","                    image_paths.append([image_path, label, category])\n","\n","    # 打乱训练/验证集图片的顺序\n","    random.shuffle(image_paths)\n","\n","    # 按8:2比例分割train和val\n","    split_index = int(len(image_paths) * 0.8)\n","    data['train'].extend(image_paths[:split_index])\n","    data['val'].extend(image_paths[split_index:])\n","\n","    # 测试集图片直接加入到test中\n","    data['test'].extend(test_image_paths)\n","\n","# 指定输出JSON文件名\n","output_json_file = '/content/drive/MyDrive/output_data.json'\n","\n","# 将数据写入JSON文件，确保中文字符被正确编码\n","with open(output_json_file, 'w', encoding='utf-8') as f:\n","    json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","print(f'JSON文件已创建: {output_json_file}')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"mount_file_id":"1hgJJE44lmLUB1EjaVGtRzrUzi88PZ2Uf","authorship_tag":"ABX9TyMaZ0fZQHvFL73CPqFAD66S"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}