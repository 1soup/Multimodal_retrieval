{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1XZVzWbmE82x-zqh7l8jA_OxXpK2Wp3Xo","authorship_tag":"ABX9TyPzjmRc9Wrcat3cJdyvsw2a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0ca0009f2a93493bac02da2a64d73e8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca0f5b4219f5403b9fb07fc481b106ce","IPY_MODEL_0cbaabb7826a46648832afb671e1f49c","IPY_MODEL_bf2a53bec0ed4e498850cfdbb8d40c73"],"layout":"IPY_MODEL_4b35cd35ba574525873d92528ccab59e"}},"ca0f5b4219f5403b9fb07fc481b106ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b001ba37765f4282bd3e70f433e0e692","placeholder":"​","style":"IPY_MODEL_37492222fac349a4828812196cc44cbf","value":"Loading checkpoint shards: 100%"}},"0cbaabb7826a46648832afb671e1f49c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc786140912243048057de4af9fec28d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef84b19574b5477f9f6fe2d7741a1eba","value":2}},"bf2a53bec0ed4e498850cfdbb8d40c73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daa6b1473a5145b9b684b143ccc4c48a","placeholder":"​","style":"IPY_MODEL_666daa6fcd174b71a05eae0005f37fea","value":" 2/2 [00:02&lt;00:00,  1.11s/it]"}},"4b35cd35ba574525873d92528ccab59e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b001ba37765f4282bd3e70f433e0e692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37492222fac349a4828812196cc44cbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc786140912243048057de4af9fec28d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef84b19574b5477f9f6fe2d7741a1eba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"daa6b1473a5145b9b684b143ccc4c48a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"666daa6fcd174b71a05eae0005f37fea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pDsvkAR8ShoO"},"outputs":[],"source":["!git clone https://github.com/haotian-liu/LLaVA.git\n","%cd LLaVA\n","!pip install -e ."]},{"cell_type":"markdown","source":["加载模型"],"metadata":{"id":"5f4Hyc4NKvC4"}},{"cell_type":"code","source":["from llava.model.builder import load_pretrained_model\n","from llava.mm_utils import get_model_name_from_path\n","from llava.eval.run_llava import eval_model\n","\n","model_path = \"liuhaotian/llava-v1.5-7b\"\n","\n","tokenizer, model, image_processor, context_len = load_pretrained_model(\n","    model_path=model_path,\n","    model_base=None,\n","    model_name=get_model_name_from_path(model_path),\n","    load_in_4bit=True\n",")"],"metadata":{"id":"yC3VhWJlV-VD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Github demo"],"metadata":{"id":"QLLrWbT5KwHR"}},{"cell_type":"code","source":["model_path = \"liuhaotian/llava-v1.5-7b\"\n","prompt = \"Please determine if this is a photo about computer? Answer yes or no\"\n","image_file = \"/content/drive/MyDrive/target_data/computer/电脑_1.png\"\n","\n","args = type('Args', (), {\n","    \"model_path\": model_path,\n","    \"model_base\": None,\n","    \"model_name\": get_model_name_from_path(model_path),\n","    \"query\": prompt,\n","    \"conv_mode\": None,\n","    \"image_file\": image_file,\n","    \"sep\": \",\",\n","    \"temperature\": 0,\n","    \"top_p\": None,\n","    \"num_beams\": 1,\n","    \"max_new_tokens\": 512\n","})()\n","\n","eval_model(args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176,"referenced_widgets":["0ca0009f2a93493bac02da2a64d73e8e","ca0f5b4219f5403b9fb07fc481b106ce","0cbaabb7826a46648832afb671e1f49c","bf2a53bec0ed4e498850cfdbb8d40c73","4b35cd35ba574525873d92528ccab59e","b001ba37765f4282bd3e70f433e0e692","37492222fac349a4828812196cc44cbf","cc786140912243048057de4af9fec28d","ef84b19574b5477f9f6fe2d7741a1eba","daa6b1473a5145b9b684b143ccc4c48a","666daa6fcd174b71a05eae0005f37fea"]},"id":"SESxcO9yWrKJ","executionInfo":{"status":"ok","timestamp":1709881595869,"user_tz":-480,"elapsed":10053,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"fcbfff10-d417-437c-a8e7-8f3e04410421"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca0009f2a93493bac02da2a64d73e8e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"markdown","source":["初始化参数"],"metadata":{"id":"xxECv6q1Kyo_"}},{"cell_type":"code","source":["import io\n","import os\n","from contextlib import redirect_stdout\n","\n","def get_eval_model_output(args):\n","    # 创建一个StringIO对象来捕获输出\n","    buffer = io.StringIO()\n","    # 重定向标准输出到buffer\n","    with redirect_stdout(buffer):\n","        # *****************注意*****************\n","        # 修改llava/eval/run_llava.py，删除每次的函数加载，节省时间\n","        eval_model(args, tokenizer, model, image_processor, context_len)\n","    # 获取buffer中的内容，即eval_model的输出\n","    output = buffer.getvalue()\n","    return output\n","\n","model_path = \"liuhaotian/llava-v1.5-7b\"\n","prompt = \"\"\n","image_file = \"\"\n","\n","args = type('Args', (), {\n","    \"model_path\": model_path,\n","    \"model_base\": None,\n","    \"model_name\": get_model_name_from_path(model_path),\n","    \"query\": prompt,\n","    \"conv_mode\": None,\n","    \"image_file\": image_file,\n","    \"sep\": \",\",\n","    \"temperature\": 0,\n","    \"top_p\": None,\n","    \"num_beams\": 1,\n","    \"max_new_tokens\": 512\n","})()"],"metadata":{"id":"mIUwYuQq3v9k","executionInfo":{"status":"ok","timestamp":1710384193540,"user_tz":-480,"elapsed":7,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["下面三段代码相同，数据不同，原因是中英文CLIP合并后得到的结果不同\n","\n","\n","*   target_data：中文CLIP没有卡阈值得到最高的精度\n","*   target_data2：英文CLIP阈值应为0.24，错选为0.23\n","*   target_data3：最终结果"],"metadata":{"id":"X6G7hjoPLtMw"}},{"cell_type":"code","source":["categories = {\n","    \"pet cat\": \"宠物猫\",\n","    \"tomato\": \"番茄\",\n","    \"paper-cut\": \"剪纸\",\n","    \"computer\": \"电脑\",\n","    \"dumpling\": \"饺子\"\n","}\n","\n","def calculate_metrics(TP, FP, FN):\n","    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","    return precision, recall, f1\n","\n","\n","def verify_and_calculate_metrics(target_root_folder, categories):\n","    total_precision, total_recall, total_f1 = 0, 0, 0\n","\n","    for category, keyword in categories.items():\n","        TP, FP, FN = 0, 0, 0\n","\n","        category_folder = os.path.join(target_root_folder, category)\n","        prompt = f\"Please determine if this is a photo about {category}? Answer yes or no\"\n","        args.query = prompt\n","\n","        for image_name in os.listdir(category_folder):\n","            image_path = os.path.join(category_folder, image_name)\n","            args.image_file = image_path\n","\n","            output = get_eval_model_output(args)\n","            is_correct = \"yes\" in output.lower()\n","\n","            # 根据图片名称判断真实类别\n","            actual_category = keyword if keyword in image_name else \"其他\"\n","\n","            # 更新统计数据\n","            if is_correct and actual_category == keyword:\n","                TP += 1\n","            elif is_correct and actual_category != keyword:\n","                FP += 1\n","\n","        # 计算并打印当前类别的Precision、Recall和F1 score\n","        FN = 200 - TP\n","        precision, recall, f1 = calculate_metrics(TP, FP, FN)\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","        print(f\"Category: {category}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # 计算并打印总体平均Precision、Recall和F1 score\n","    average_precision = total_precision / len(categories)\n","    average_recall = total_recall / len(categories)\n","    average_f1 = total_f1 / len(categories)\n","    print(f\"Average Precision: {average_precision:.4f}, Average Recall: {average_recall:.4f}, Average F1: {average_f1:.4f}\")\n","\n","\n","# 示例调用函数\n","target_root_folder = \"/content/drive/MyDrive/target_data\"\n","verify_and_calculate_metrics(target_root_folder, categories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9Sb5fJlLJp7","executionInfo":{"status":"ok","timestamp":1709893627937,"user_tz":-480,"elapsed":1242020,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"2b474ee8-fbca-43c4-8919-38bc72e72d43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Category: pet cat, Precision: 0.9132, Recall: 1.0000, F1 Score: 0.9547\n","Category: tomato, Precision: 0.9949, Recall: 0.9750, F1 Score: 0.9848\n","Category: paper-cut, Precision: 0.8609, Recall: 0.9900, F1 Score: 0.9209\n","Category: computer, Precision: 0.9469, Recall: 0.9800, F1 Score: 0.9631\n","Category: dumpling, Precision: 0.7976, Recall: 0.9850, F1 Score: 0.8814\n","Average Precision: 0.9027, Average Recall: 0.9860, Average F1: 0.9410\n"]}]},{"cell_type":"code","source":["categories = {\n","    \"pet cat\": \"宠物猫\",\n","    \"tomato\": \"番茄\",\n","    \"paper-cut\": \"剪纸\",\n","    \"computer\": \"电脑\",\n","    \"dumpling\": \"饺子\"\n","}\n","\n","def calculate_metrics(TP, FP, FN):\n","    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","    return precision, recall, f1\n","\n","\n","def verify_and_calculate_metrics(target_root_folder, categories):\n","    total_precision, total_recall, total_f1 = 0, 0, 0\n","\n","    for category, keyword in categories.items():\n","        TP, FP, FN = 0, 0, 0\n","\n","        category_folder = os.path.join(target_root_folder, category)\n","        prompt = f\"Please determine if this is a photo about {category}? Answer yes or no\"\n","        args.query = prompt\n","\n","        for image_name in os.listdir(category_folder):\n","            image_path = os.path.join(category_folder, image_name)\n","            args.image_file = image_path\n","\n","            output = get_eval_model_output(args)\n","            is_correct = \"yes\" in output.lower()\n","\n","            # 根据图片名称判断真实类别\n","            actual_category = keyword if keyword in image_name else \"其他\"\n","\n","            # 更新统计数据\n","            if is_correct and actual_category == keyword:\n","                TP += 1\n","            elif is_correct and actual_category != keyword:\n","                FP += 1\n","\n","        # 计算并打印当前类别的Precision、Recall和F1 score\n","        FN = 200 - TP\n","        precision, recall, f1 = calculate_metrics(TP, FP, FN)\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","        print(f\"Category: {category}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # 计算并打印总体平均Precision、Recall和F1 score\n","    average_precision = total_precision / len(categories)\n","    average_recall = total_recall / len(categories)\n","    average_f1 = total_f1 / len(categories)\n","    print(f\"Average Precision: {average_precision:.4f}, Average Recall: {average_recall:.4f}, Average F1: {average_f1:.4f}\")\n","\n","\n","# 示例调用函数\n","target_root_folder = \"/content/drive/MyDrive/target_data2\"\n","verify_and_calculate_metrics(target_root_folder, categories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyVaTEvlSsGH","executionInfo":{"status":"ok","timestamp":1710242624659,"user_tz":-480,"elapsed":1156728,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"2cf4f580-815f-4bee-c6aa-a8f44810b21f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Category: pet cat, Precision: 0.9615, Recall: 1.0000, F1 Score: 0.9804\n","Category: tomato, Precision: 0.9949, Recall: 0.9750, F1 Score: 0.9848\n","Category: paper-cut, Precision: 0.8603, Recall: 0.9850, F1 Score: 0.9184\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Category: computer, Precision: 0.9897, Recall: 0.9600, F1 Score: 0.9746\n","Category: dumpling, Precision: 0.8140, Recall: 0.9850, F1 Score: 0.8914\n","Average Precision: 0.9241, Average Recall: 0.9810, Average F1: 0.9499\n"]}]},{"cell_type":"code","source":["categories = {\n","    \"pet cat\": \"宠物猫\",\n","    \"tomato\": \"番茄\",\n","    \"paper-cut\": \"剪纸\",\n","    \"computer\": \"电脑\",\n","    \"dumpling\": \"饺子\"\n","}\n","\n","def calculate_metrics(TP, FP, FN):\n","    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","    return precision, recall, f1\n","\n","\n","def verify_and_calculate_metrics(target_root_folder, categories):\n","    total_precision, total_recall, total_f1 = 0, 0, 0\n","\n","    for category, keyword in categories.items():\n","        TP, FP, FN = 0, 0, 0\n","\n","        category_folder = os.path.join(target_root_folder, category)\n","        prompt = f\"Please determine if this is a photo about {category}? Answer yes or no\"\n","        args.query = prompt\n","\n","        for image_name in os.listdir(category_folder):\n","            image_path = os.path.join(category_folder, image_name)\n","            args.image_file = image_path\n","\n","            output = get_eval_model_output(args)\n","            is_correct = \"yes\" in output.lower()\n","\n","            # 根据图片名称判断真实类别\n","            actual_category = keyword if keyword in image_name else \"其他\"\n","\n","            # 更新统计数据\n","            if is_correct and actual_category == keyword:\n","                TP += 1\n","            elif is_correct and actual_category != keyword:\n","                FP += 1\n","\n","        # 计算并打印当前类别的Precision、Recall和F1 score\n","        FN = 200 - TP\n","        precision, recall, f1 = calculate_metrics(TP, FP, FN)\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","        print(f\"Category: {category}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # 计算并打印总体平均Precision、Recall和F1 score\n","    average_precision = total_precision / len(categories)\n","    average_recall = total_recall / len(categories)\n","    average_f1 = total_f1 / len(categories)\n","    print(f\"Average Precision: {average_precision:.4f}, Average Recall: {average_recall:.4f}, Average F1: {average_f1:.4f}\")\n","\n","\n","# 示例调用函数\n","target_root_folder = \"/content/drive/MyDrive/target_data3\"\n","verify_and_calculate_metrics(target_root_folder, categories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zx3aSVuZzq5e","executionInfo":{"status":"ok","timestamp":1710385138663,"user_tz":-480,"elapsed":75563,"user":{"displayName":"Thomasine Kaczka","userId":"06732724743998191518"}},"outputId":"a27b5ac5-a13f-4f09-d8e3-49c8ced5d261"},"execution_count":3,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Category: pet cat, Precision: 0.9662, Recall: 1.0000, F1 Score: 0.9828\n","Category: tomato, Precision: 0.9949, Recall: 0.9750, F1 Score: 0.9848\n","Category: paper-cut, Precision: 0.9336, Recall: 0.9850, F1 Score: 0.9586\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Category: computer, Precision: 1.0000, Recall: 0.9150, F1 Score: 0.9556\n","Category: dumpling, Precision: 0.8383, Recall: 0.9850, F1 Score: 0.9057\n","Average Precision: 0.9466, Average Recall: 0.9720, Average F1: 0.9575\n"]}]},{"cell_type":"markdown","source":["释放内存"],"metadata":{"id":"QgZpS_hLNCU5"}},{"cell_type":"code","source":["del model"],"metadata":{"id":"SQ-LH_vmdbFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"vmhUHmS6h9oV"},"execution_count":null,"outputs":[]}]}