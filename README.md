# 基于多模态检索的互联网图文匹配
## 运行环境
Google Colaboratory  
LLaVa部分需要A100-40G，其余16G即可
## 背景
图文检索是当前最重要的多模态能力之一，针对互联网或者AI公司积累的大量历史数据，以及手机图库中的图片数据，利用多模态大模型对这些数据进行检索，是数据治理的关键。
## 内容
- 数据来源：爬取多类别的互联网图像数据2000张（正负样本类别各5）
- 主要流程：中英文CLIP图文匹配+多模态大模型LLaVA结果矫正
- 利用CLIP中英文双语模型对图片以及文本标签进行匹配，并通过对比实验，找到精度最高的计算方法，即通过卡阈值计算相似度，分别得到中英文CLIP的图文匹配结果。对中英文CLIP的结果进行整合，再通过多模态大模型LLaVA的VQA对匹配结果进行矫正，得到最终的图片分类结果。此外，还尝试采用CLIP-Adapter的方法，对CLIP模型进行finetune。
## 结论
相比于原始CLIP模型，中英文模型合并的方式不损失Precison，将Recall提高了11.7%。通过LLaVA矫正，损失少量Recall，大幅提高Precison，最终精度(F1 Score)提高了19.9%。少量图片的Clip-adapter微调也提高了2.93%的精度。
